{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***GPT FROM SCRATCH: CHAR LEVEL GPT-2***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gpt.bigram import BigramModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input file: all Shakespeare books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open(\"input.txt\", 'r', encoding='utf-8').read()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary and encode/decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=65\n",
      "chars=['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] \n",
      "\n",
      "[43, 52, 41, 53, 42, 43, 1, 58, 46, 47, 57, 1, 58, 43, 62, 58]\n",
      "encode this text\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"{vocab_size=}\\n{chars=} \\n\")\n",
    "\n",
    "#Create a dictionary mapping each character to a unique integer\n",
    "stoi = {ch: i for i,ch in enumerate(chars)}\n",
    "itos = {v:k for k,v in stoi.items()}\n",
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(encode(\"encode this text\"))\n",
    "print(decode(encode(\"encode this text\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "def train_test_val_split(data: str,\n",
    "                         train_size: float = 0.8,\n",
    "                         val_size: float = 0.1,\n",
    "                         test_size: float = 0.1):\n",
    "    assert train_size + val_size + test_size == 1\n",
    "    data_len = len(data)\n",
    "    train_len = int(data_len * train_size)\n",
    "    val_len = int(data_len * val_size)\n",
    "    test_len = data_len - train_len - val_len\n",
    "    train_data = data[:train_len]\n",
    "    val_data = data[train_len:train_len+val_len]\n",
    "    test_data = data[train_len+val_len:]\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "train_data, val_data, test_data = train_test_val_split(data, 0.8, 0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "torch.manual_seed(42)\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take batches of len of block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb=tensor([[47, 57, 10,  1, 39, 52, 42,  1],\n",
      "        [59, 56,  1, 46, 43, 39, 56, 58],\n",
      "        [32, 46, 39, 58,  1, 39, 50, 61],\n",
      "        [26, 53, 58, 46, 47, 52, 45,  1]])\n",
      "yb=tensor([[57, 10,  1, 39, 52, 42,  1, 50],\n",
      "        [56,  1, 46, 43, 39, 56, 58, 57],\n",
      "        [46, 39, 58,  1, 39, 50, 61, 39],\n",
      "        [53, 58, 46, 47, 52, 45,  1, 40]])\n",
      "when input is [47] the target: 57\n",
      "when input is [47, 57] the target: 10\n",
      "when input is [47, 57, 10] the target: 1\n",
      "when input is [47, 57, 10, 1] the target: 39\n",
      "when input is [47, 57, 10, 1, 39] the target: 52\n",
      "when input is [47, 57, 10, 1, 39, 52] the target: 42\n",
      "when input is [47, 57, 10, 1, 39, 52, 42] the target: 1\n",
      "when input is [47, 57, 10, 1, 39, 52, 42, 1] the target: 50\n",
      "when input is [59] the target: 56\n",
      "when input is [59, 56] the target: 1\n",
      "when input is [59, 56, 1] the target: 46\n",
      "when input is [59, 56, 1, 46] the target: 43\n",
      "when input is [59, 56, 1, 46, 43] the target: 39\n",
      "when input is [59, 56, 1, 46, 43, 39] the target: 56\n",
      "when input is [59, 56, 1, 46, 43, 39, 56] the target: 58\n",
      "when input is [59, 56, 1, 46, 43, 39, 56, 58] the target: 57\n",
      "when input is [32] the target: 46\n",
      "when input is [32, 46] the target: 39\n",
      "when input is [32, 46, 39] the target: 58\n",
      "when input is [32, 46, 39, 58] the target: 1\n",
      "when input is [32, 46, 39, 58, 1] the target: 39\n",
      "when input is [32, 46, 39, 58, 1, 39] the target: 50\n",
      "when input is [32, 46, 39, 58, 1, 39, 50] the target: 61\n",
      "when input is [32, 46, 39, 58, 1, 39, 50, 61] the target: 39\n",
      "when input is [26] the target: 53\n",
      "when input is [26, 53] the target: 58\n",
      "when input is [26, 53, 58] the target: 46\n",
      "when input is [26, 53, 58, 46] the target: 47\n",
      "when input is [26, 53, 58, 46, 47] the target: 52\n",
      "when input is [26, 53, 58, 46, 47, 52] the target: 45\n",
      "when input is [26, 53, 58, 46, 47, 52, 45] the target: 1\n",
      "when input is [26, 53, 58, 46, 47, 52, 45, 1] the target: 40\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"{xb=}\")\n",
    "print(f\"{yb=}\")\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram char-level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Embedding table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: (batch_size, block_size)\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # Channel must b 2nd dim\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # Only last prediction\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Takes one new token and add it to the idx\n",
    "            idx = torch.cat([idx, torch.multinomial(probs, num_samples=1)], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F$z\n",
      "E?kFwu\n",
      "'buM-,Y3fYNsA3xp.mpsAqaZ-RUldc;F\n",
      "M$GfYCUCkFO-bJbz-R;O!slp.FNsJDV'jRzIMQ'EdRbqAoWTjrniaIIa\n"
     ]
    }
   ],
   "source": [
    "m = BigramModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long),\n",
    "                        max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.702682971954346\n",
      ", how I U?OlqbAoCjULUlwyVfGdsujrfc-j&Br,VkvPT ;mVDFNvPNvH-EsAa$dJTVwrx fK-d3fluagUuTVlsN.d?VLUlvIeNsxqaIOlVE\n",
      "Loss: 2.7912943363189697\n",
      "t wrapt bOhyVYotoun:JbJy, budutUSbr!OMJ, O-\n",
      "Pr\n",
      "\n",
      "O:liM3bkLADUy.OI wedeTOR?\n",
      "HA;\n",
      "\n",
      "\n",
      "CKaSHARGENIZcy onoA\n",
      "Tourel-k\n",
      "Loss: 2.5110576152801514\n",
      "ur highnchJ! hour, w wIIfr!\n",
      "Wharersw s ayo yo d ;\n",
      "\n",
      "\n",
      "wet charilkse thtwkimburang bendetharsRf t reix ncWhevev\n",
      "Loss: 2.283414840698242\n",
      "t,\n",
      "And hon tr out irerond; ben, squa cinol,\n",
      "f! wind me ar he pliear, RDUTh t, My ffourere y. w\n",
      "S:\n",
      "An\n",
      "CKerd y\n",
      "Loss: 2.3937628269195557\n",
      "as,\n",
      "Thanodore, METhitHEEShoue ard orelliver uran, hillvr, shied t?\n",
      "\n",
      "Anjergngery-ugl alve ithad thyoupesspan.\n",
      "Loss: 2.504157781600952\n",
      "oung affil CI'e d arnonth'tegin as s y ar blveandave, p my ghasouririn nincrind frds staicand t blis d, outa\n",
      "Loss: 2.4056339263916016\n",
      "ard, not jeow meeeinge me.\n",
      "Shatourgh s foor more? k?\n",
      "Whe sed.\n",
      "Th t?\n",
      "OHarevertaseviod d, be free telouk-ingm \n",
      "Loss: 2.3617005348205566\n",
      "I hope hmefimes.\n",
      "D:\n",
      "\n",
      "\n",
      "\n",
      "HAGERIf fy Lo btsco, m.\n",
      "\n",
      "Tar eneat acerine, lese tor n! Here the wo sha douck oupamak\n",
      "Loss: 2.3808746337890625\n",
      "rking inglloth ild; st mede, fithissctoutrod'd wanth kirthy y r athawathoul m d youtro h'socos d\n",
      "TED:\n",
      "Wileve\n",
      "Loss: 2.579768180847168\n",
      "tune shayoouossigs southill at tos HArd llondelllllakende, knef alles mbleld he avigisehe,\n",
      "UCEOLAs Thed, gha\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.AdamW(m.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 32\n",
    "n_steps = 30000\n",
    "for _ in range(n_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if _ % (n_steps//10) == 0:\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        print(decode(m.generate(idx = xb[:1], max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention Mecanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei.shape=torch.Size([4, 8, 8])\n",
      "torch.Size([4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B,T,C = 4,8,2 # batch, time, channel\n",
    "trill = torch.tril(torch.ones(T, T)) # Low Triangular matrix (decode)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Attention Head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "# Key, Query and Value\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "\n",
    "# Attention\n",
    "wei = q @ k.transpose(-2, -1) / head_size**0.5\n",
    "print(f\"{wei.shape=}\")\n",
    "wei = wei.masked_fill(trill == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# wei = trill @ wei\n",
    "# wei = wei.mean(dim=-1, keepdim=True)\n",
    "print(wei.shape)\n",
    "output = wei @ v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
